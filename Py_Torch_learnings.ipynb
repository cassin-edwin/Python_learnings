{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5223e-44, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating an empty tensor\n",
    "x = torch.empty(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5876, 0.6234],\n",
       "        [0.4203, 0.6031],\n",
       "        [0.6814, 0.1725]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a rand tensor\n",
    "x = torch.rand(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a tensor with ones\n",
    "x = torch.ones(3,2, dtype = torch.int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2422, 1.2734],\n",
      "        [0.8408, 1.0964],\n",
      "        [0.4856, 0.6490]])\n",
      "0.48563194274902344\n",
      "tensor([[1.2422, 1.2734, 0.8408],\n",
      "        [1.0964, 0.4856, 0.6490]])\n",
      "torch.Size([2, 3])\n",
      "[[1.2422497  1.2733896 ]\n",
      " [0.8407612  1.0964413 ]\n",
      " [0.48563194 0.64898837]] <class 'numpy.ndarray'>\n",
      "[1 2 3 4 5]\n",
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "#Addition of two tensors\n",
    "x = torch.rand(3,2)\n",
    "y = torch.rand(3,2)\n",
    "z = x+y\n",
    "z\n",
    "\"\"\"or\"\"\"\n",
    "#Similar for other operations sub(),mul(),div()\n",
    "z = torch.add(x,y)\n",
    "print(z)\n",
    "\n",
    "#To print an element as an individual item\n",
    "print(z[2,0].item())\n",
    "\n",
    "#To view it in different dimensions\n",
    "print(z.view(2,3))\n",
    "\n",
    "#To view the correct dimension \n",
    "print(z.view(2,-1).size())\n",
    "\n",
    "#Convert tensors into numpy arrays\n",
    "y = z.numpy()\n",
    "print(y,type(y))\n",
    "\n",
    "#Convert numpy arrays into tensors\n",
    "import numpy as np\n",
    "v = np.array([1,2,3,4,5])\n",
    "print(v)\n",
    "w = torch.from_numpy(v)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    #If gpu is available, this will execute\n",
    "    device = torch.device('conda')\n",
    "    x = tensor.ones(5,device = device)\n",
    "    y = tensor.ones(5)\n",
    "    y.to(device)\n",
    "    z = x+y\n",
    "    print(z.to('cpu'))\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2233, 2.5919, 0.1686], requires_grad=True)\n",
      "tensor([-0.5808,  0.4780, -1.4180], requires_grad=True)\n",
      "tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.1936,  0.1593, -0.4727])\n",
      "tensor([0.4078, 0.8640, 0.0562])\n",
      "tensor([1.2233, 2.5919, 0.1686])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = torch.randn(3,requires_grad = True)\n",
    "print(y)\n",
    "\n",
    "z = (x*y).mean()\n",
    "print(z)\n",
    "\n",
    "#Backward propagation to create gradient values for x and y\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n",
    "#Removing the condition -> requires_grad = False\n",
    "x = x.requires_grad_(False)\n",
    "\"\"\"or\"\"\"\n",
    "x = x.detach()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad = True)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_output = x.mean()\n",
    "    model_output.backward()\n",
    "    print(x.grad)\n",
    "    #**Very Important**\n",
    "    #After every iteration, the gradient values of a variable should be made to 0 else the values would get added up. \n",
    "    x.grad.zero_()\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: = [0.1, 0.4, 0.90000004, 1.6]\n",
      "20.885\n",
      "epoch 1: w = [1.1 1.2 1.3 1.4], loss = 20.885000228881836\n",
      "3.3850002\n",
      "epoch 2: w = [1.5       1.6       1.6999999 1.8      ], loss = 3.385000228881836\n",
      "0.5850001\n",
      "epoch 3: w = [1.66      1.76      1.8599999 1.9599999], loss = 0.5850000977516174\n",
      "0.13700004\n",
      "epoch 4: w = [1.724     1.824     1.9239999 2.024    ], loss = 0.13700003921985626\n",
      "0.06532001\n",
      "epoch 5: w = [1.7496    1.8496001 1.9496    2.0496   ], loss = 0.06532000750303268\n",
      "0.05385115\n",
      "epoch 6: w = [1.7598401 1.8598402 1.95984   2.05984  ], loss = 0.05385114997625351\n",
      "0.052016087\n",
      "epoch 7: w = [1.763936  1.8639362 1.9639361 2.063936 ], loss = 0.052016086876392365\n",
      "0.051722504\n",
      "epoch 8: w = [1.7655745 1.8655746 1.9655745 2.0655744], loss = 0.05172250419855118\n",
      "0.051675532\n",
      "epoch 9: w = [1.7662297 1.8662299 1.9662298 2.0662298], loss = 0.05167553201317787\n",
      "0.051668093\n",
      "epoch 10: w = [1.7664919 1.866492  1.9664919 2.0664918], loss = 0.05166809260845184\n",
      "Prediction after training : = [1.7664919, 3.732984, 5.899476, 8.265967]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Forward and Backward passes along with updating weights\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "Y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "w = np.array([0.1,0.2,0.3,0.4], dtype = np.float32)\n",
    "\n",
    "def forward(w,x):\n",
    "    a = []\n",
    "    for i in range(0,len(X)):\n",
    "        a.append(X[i]*w[i])\n",
    "    return a\n",
    "\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "def gradient(x,y,y_pred):\n",
    "    \n",
    "    return (np.dot(2*x,y_pred-y)).mean()\n",
    "\n",
    "print(f'Prediction before training: = {forward(w,X)}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(w,X)\n",
    "    \n",
    "    l = loss(Y, y_pred)\n",
    "    print(l)\n",
    "    dw = gradient(X,Y, y_pred)\n",
    "  \n",
    "    for i in range(0,len(y_pred)):\n",
    "        w[i] = w[i] - learning_rate*dw\n",
    "    \n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w}, loss = {l}')\n",
    "\n",
    "print(f'Prediction after training : = {forward(w,X)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.050\n",
      "tensor(-29.8500)\n",
      "epoch 1: w = 0.3084999918937683, loss = 29.700748443603516\n",
      "tensor(-25.3725)\n",
      "epoch 2: w = 0.5622249841690063, loss = 21.458789825439453\n",
      "tensor(-21.5666)\n",
      "epoch 3: w = 0.777891218662262, loss = 15.50397777557373\n",
      "tensor(-18.3316)\n",
      "epoch 4: w = 0.9612075090408325, loss = 11.201624870300293\n",
      "tensor(-15.5819)\n",
      "epoch 5: w = 1.1170263290405273, loss = 8.09317398071289\n",
      "tensor(-13.2446)\n",
      "epoch 6: w = 1.2494723796844482, loss = 5.847318649291992\n",
      "tensor(-11.2579)\n",
      "epoch 7: w = 1.3620514869689941, loss = 4.2246880531311035\n",
      "tensor(-9.5692)\n",
      "epoch 8: w = 1.457743763923645, loss = 3.052337408065796\n",
      "tensor(-8.1338)\n",
      "epoch 9: w = 1.5390821695327759, loss = 2.2053136825561523\n",
      "tensor(-6.9138)\n",
      "epoch 10: w = 1.608219861984253, loss = 1.593339204788208\n",
      "Prediction after training: f(5) = 8.041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Forward and Backward passes along with updating weights\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.01, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "def gradient(x,y,y_pred):\n",
    "    \n",
    "    return (np.dot(2*x,y_pred-y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    #Using backward() function\n",
    "    l.backward()\n",
    "    print(w.grad)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "        \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w}, loss = {l}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 3.632\n",
      "epoch 1: w = 0.000, loss = 12.17042923\n",
      "epoch 2: w = 0.000, loss = 8.45229340\n",
      "epoch 3: w = 0.000, loss = 5.87231302\n",
      "epoch 4: w = 0.000, loss = 4.08207512\n",
      "epoch 5: w = 0.000, loss = 2.83982205\n",
      "epoch 6: w = 0.000, loss = 1.97780490\n",
      "epoch 7: w = 0.000, loss = 1.37962627\n",
      "epoch 8: w = 0.000, loss = 0.96451867\n",
      "epoch 9: w = 0.000, loss = 0.67644191\n",
      "epoch 10: w = 0.000, loss = 0.47650814\n",
      "Prediction after training : f(5) = 8.756\n"
     ]
    }
   ],
   "source": [
    "#Implementation of Linear Regression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features \n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size,output_size)\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #Prediction\n",
    "    y_pred = model.forward(X)\n",
    "    \n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    #gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        #Using step() function to update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    #Using zero_grad comman\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss = 0.5903\n",
      "epoch : 2, loss = 0.5777\n",
      "epoch : 3, loss = 0.5659\n",
      "epoch : 4, loss = 0.5546\n",
      "epoch : 5, loss = 0.5440\n",
      "epoch : 6, loss = 0.5338\n",
      "epoch : 7, loss = 0.5242\n",
      "epoch : 8, loss = 0.5151\n",
      "epoch : 9, loss = 0.5064\n",
      "epoch : 10, loss = 0.4981\n",
      "epoch : 11, loss = 0.4902\n",
      "epoch : 12, loss = 0.4827\n",
      "epoch : 13, loss = 0.4754\n",
      "epoch : 14, loss = 0.4685\n",
      "epoch : 15, loss = 0.4619\n",
      "epoch : 16, loss = 0.4556\n",
      "epoch : 17, loss = 0.4495\n",
      "epoch : 18, loss = 0.4436\n",
      "epoch : 19, loss = 0.4380\n",
      "epoch : 20, loss = 0.4326\n",
      "epoch : 21, loss = 0.4274\n",
      "epoch : 22, loss = 0.4223\n",
      "epoch : 23, loss = 0.4175\n",
      "epoch : 24, loss = 0.4128\n",
      "epoch : 25, loss = 0.4082\n",
      "epoch : 26, loss = 0.4039\n",
      "epoch : 27, loss = 0.3996\n",
      "epoch : 28, loss = 0.3955\n",
      "epoch : 29, loss = 0.3916\n",
      "epoch : 30, loss = 0.3877\n",
      "epoch : 31, loss = 0.3840\n",
      "epoch : 32, loss = 0.3803\n",
      "epoch : 33, loss = 0.3768\n",
      "epoch : 34, loss = 0.3734\n",
      "epoch : 35, loss = 0.3700\n",
      "epoch : 36, loss = 0.3668\n",
      "epoch : 37, loss = 0.3637\n",
      "epoch : 38, loss = 0.3606\n",
      "epoch : 39, loss = 0.3576\n",
      "epoch : 40, loss = 0.3547\n",
      "epoch : 41, loss = 0.3518\n",
      "epoch : 42, loss = 0.3491\n",
      "epoch : 43, loss = 0.3464\n",
      "epoch : 44, loss = 0.3437\n",
      "epoch : 45, loss = 0.3412\n",
      "epoch : 46, loss = 0.3386\n",
      "epoch : 47, loss = 0.3362\n",
      "epoch : 48, loss = 0.3338\n",
      "epoch : 49, loss = 0.3314\n",
      "epoch : 50, loss = 0.3291\n",
      "epoch : 51, loss = 0.3269\n",
      "epoch : 52, loss = 0.3247\n",
      "epoch : 53, loss = 0.3225\n",
      "epoch : 54, loss = 0.3204\n",
      "epoch : 55, loss = 0.3184\n",
      "epoch : 56, loss = 0.3163\n",
      "epoch : 57, loss = 0.3143\n",
      "epoch : 58, loss = 0.3124\n",
      "epoch : 59, loss = 0.3105\n",
      "epoch : 60, loss = 0.3086\n",
      "epoch : 61, loss = 0.3068\n",
      "epoch : 62, loss = 0.3050\n",
      "epoch : 63, loss = 0.3032\n",
      "epoch : 64, loss = 0.3015\n",
      "epoch : 65, loss = 0.2998\n",
      "epoch : 66, loss = 0.2981\n",
      "epoch : 67, loss = 0.2965\n",
      "epoch : 68, loss = 0.2948\n",
      "epoch : 69, loss = 0.2932\n",
      "epoch : 70, loss = 0.2917\n",
      "epoch : 71, loss = 0.2901\n",
      "epoch : 72, loss = 0.2886\n",
      "epoch : 73, loss = 0.2871\n",
      "epoch : 74, loss = 0.2857\n",
      "epoch : 75, loss = 0.2842\n",
      "epoch : 76, loss = 0.2828\n",
      "epoch : 77, loss = 0.2814\n",
      "epoch : 78, loss = 0.2801\n",
      "epoch : 79, loss = 0.2787\n",
      "epoch : 80, loss = 0.2774\n",
      "epoch : 81, loss = 0.2761\n",
      "epoch : 82, loss = 0.2748\n",
      "epoch : 83, loss = 0.2735\n",
      "epoch : 84, loss = 0.2723\n",
      "epoch : 85, loss = 0.2710\n",
      "epoch : 86, loss = 0.2698\n",
      "epoch : 87, loss = 0.2686\n",
      "epoch : 88, loss = 0.2674\n",
      "epoch : 89, loss = 0.2663\n",
      "epoch : 90, loss = 0.2651\n",
      "epoch : 91, loss = 0.2640\n",
      "epoch : 92, loss = 0.2629\n",
      "epoch : 93, loss = 0.2618\n",
      "epoch : 94, loss = 0.2607\n",
      "epoch : 95, loss = 0.2596\n",
      "epoch : 96, loss = 0.2585\n",
      "epoch : 97, loss = 0.2575\n",
      "epoch : 98, loss = 0.2565\n",
      "epoch : 99, loss = 0.2554\n",
      "epoch : 100, loss = 0.2544\n",
      "accuracy = 0.964912\n"
     ]
    }
   ],
   "source": [
    "#Implementation of Logistic Regression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X,y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_input_features):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.lin = nn.Linear(n_input_features,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_predicted = torch.sigmoid(self.lin(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'epoch : {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_predicted_cls = y_pred.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #data loading\n",
    "        xy = np.loadtxt('/Users/cassinthangam/Desktop/wine.csv', delimiter = \",\", dtype = np.float32, skiprows = 1)\n",
    "        self.x = torch.from_numpy(xy[:,1:])\n",
    "        self.y = torch.from_numpy(xy[:,[0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True)\n",
    "\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs,labels) in enumerate(dataloader):\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
